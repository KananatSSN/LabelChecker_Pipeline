{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analyze a Classification Model Using Labelchecker Data**\n",
    "Here, we walk look at the classification performance of a trained model using the small dataset provided.\n",
    "\n",
    "Hereâ€™s an overview of what weâ€™ll cover:\n",
    "\n",
    "1. **Data Download**: Obtain the example data.\n",
    "2. **Data Preparation**: Detail the necessary processing steps before analysis.\n",
    "3. **Model download**: Download the model.\n",
    "4. **Model loading**: Load the model for analyzes.\n",
    "5. **Data Loading**: Set up data loaders for model prediction.\n",
    "6. **Model Evaluation**: Assess its performance.\n",
    "\n",
    "Feel free to replace our example data and model with your own ðŸ˜Ž. Letâ€™s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 **import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "from rich import print\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from plotly import express as px\n",
    "from plotly import graph_objects as go\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.schemas.ModelConfig import ModelConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Data Download**\n",
    "Let's download the example data and start exploring it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the data\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "# set dataset name\n",
    "dataset_name = \"example\"\n",
    "data_path = data_path.joinpath(dataset_name)\n",
    "# make sure the data directory and subdirectories exists\n",
    "data_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data zip file\n",
    "data_url = \"https://zenodo.org/records/14755172/files/data.zip\"\n",
    "data_file = data_path.joinpath(\"data.zip\")\n",
    "\n",
    "if not data_file.exists():\n",
    "    print(f\"Downloading data from {data_url}\")\n",
    "    r = requests.get(data_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        # get total file size\n",
    "        total_size = int(r.headers.get('content-length', 0))\n",
    "        \n",
    "        # save the data to a file with progress bar\n",
    "        with open(data_file, 'wb') as f, tqdm(\n",
    "            desc=\"Downloading\",\n",
    "            total=total_size,\n",
    "            unit='iB',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as pbar:\n",
    "            for input_data_samples in r.iter_content(chunk_size=1024):\n",
    "                size = f.write(input_data_samples)\n",
    "                pbar.update(size)\n",
    "        \n",
    "        # extract the data with progress bar\n",
    "        with zipfile.ZipFile(data_file, 'r') as zip_ref:\n",
    "            members = zip_ref.namelist()\n",
    "            for member in tqdm(members, desc=\"Extracting\"):\n",
    "                zip_ref.extract(member, data_path)\n",
    "        print(f\"Data extracted to {data_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download data: {r.status_code}\")\n",
    "else:\n",
    "    print(f\"Data file {data_file} already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all Labelchecker data files from the data directory\n",
    "data_files = list(data_path.glob(f\"**/LabelChecker_*.csv\"))\n",
    "print(f\"Found {len(data_files)} data files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Data Preparation**\n",
    "To clean and prepare the data we:\n",
    "1. Subset data that has a `LabelTrue value`\n",
    "2. drop columns with only `missing values`\n",
    "3. drop columns with `default values`\n",
    "4. set `image paths`\n",
    "5. drop columns with `object` data\n",
    "6. remove labels with less than N examples\n",
    "\n",
    "> for details see about the data preprocessing steps see [train_model.ipynb](./train_model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for default values function\n",
    "def is_default(series: pd.Series) -> bool:\n",
    "    return len(series.unique()) == 1\n",
    "\n",
    "\n",
    "# drop all object columns except for LabelTrue function\n",
    "def is_object(\n",
    "    series: pd.Series,\n",
    "    columns_to_keep: list[str] = [\"LabelTrue\", \"ImageFilename\", \"CollageFile\"],\n",
    ") -> bool:\n",
    "    if series.name in columns_to_keep:\n",
    "        return False\n",
    "    return series.dtype == \"object\"\n",
    "\n",
    "# drop labels with less than N examples\n",
    "def drop_labels_with_less_than_examples(data: pd.DataFrame, min_examples: int) -> pd.DataFrame:\n",
    "    return data.groupby(\"LabelTrue\").filter(lambda x: len(x) >= min_examples)\n",
    "\n",
    "# build image paths\n",
    "def build_image_path(df: pd.DataFrame, directory: Path) -> Tuple[bool, list[str]]:\n",
    "    \"\"\"\n",
    "    Builds a list of image paths based on the given DataFrame and directory.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the image filenames and names.\n",
    "        directory (Path): The directory where the images are located.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[bool, list[str]]: A tuple containing a boolean value indicating whether the image paths are for collage files,\n",
    "        and a list of image paths.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If any of the image files are missing.\n",
    "    \"\"\"\n",
    "    is_collage = True\n",
    "    image_paths = []\n",
    "    if \"ImageFilename\" in df.columns:\n",
    "        if not df[\"ImageFilename\"].isnull().all() and not df[\"Name\"].isnull().all():\n",
    "            is_collage = False\n",
    "            for name, filename in zip(df[\"Name\"], df[\"ImageFilename\"]):\n",
    "                image_path = Path.joinpath(directory, name, filename)\n",
    "                if not image_path.exists():\n",
    "                    raise FileNotFoundError(f\"file {filename} not found\")\n",
    "                image_paths.append(image_path.as_posix())\n",
    "    if \"CollageFile\" in df.columns:\n",
    "        if not df[\"CollageFile\"].isnull().all():\n",
    "            is_collage = True\n",
    "            for collage_file in df[\"CollageFile\"]:\n",
    "                image_path = Path.joinpath(directory, collage_file)\n",
    "                if not image_path.exists():\n",
    "                    raise FileNotFoundError(f\"file {collage_file} not found\")\n",
    "                image_paths.append(image_path.as_posix())\n",
    "    return is_collage, image_paths\n",
    "\n",
    "\n",
    "def load_and_process_data(\n",
    "    data_files: list[Path], \n",
    ") -> Tuple[pd.DataFrame, LabelEncoder]:\n",
    "    \"\"\"\n",
    "    Load data from the data files and preprocess the data.\n",
    "\n",
    "    Args:\n",
    "        data_files (list[Path]): A list of file paths to the data files.\n",
    "        min_examples (int, optional): The minimum number of examples required for each label. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A tuple containing the preprocessed data as a DataFrame\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for data_file in data_files:\n",
    "        if not data_file.exists():\n",
    "            raise FileNotFoundError(f\"File {data_file} not found\")\n",
    "        df = pd.read_csv(data_file)\n",
    "\n",
    "        # Build the image paths\n",
    "        is_collage, image_paths = build_image_path(df, data_file.parent)\n",
    "        if image_paths:\n",
    "            if is_collage:\n",
    "                df[\"CollageFile\"] = image_paths\n",
    "            else:\n",
    "                df[\"ImageFilename\"] = image_paths\n",
    "        data.append(df)\n",
    "    data = pd.concat(data)\n",
    "\n",
    "    # Drop rows with missing LabelTrue values\n",
    "    data = data.loc[data[\"LabelTrue\"].str.len() > 0]\n",
    "    data = data.dropna(subset=[\"LabelTrue\"])\n",
    "\n",
    "    # Drop columns with all missing values\n",
    "    data = data.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Drop columns with default values\n",
    "    data = data.loc[:, ~data.apply(is_default)]\n",
    "\n",
    "    # Drop all object columns except for LabelTrue function\n",
    "    data = data.loc[:, ~data.apply(is_object)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "input_data_samples = load_and_process_data(data_files)\n",
    "print(f\"the data contains {input_data_samples.shape[0]} samples\")\n",
    "print(\n",
    "    f\"the data contains the following columns: {[column_name for column_name in input_data_samples.columns]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Model downloading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to service\n",
    "path_to_service = Path().joinpath(\"src\", \"services\", \"classification\")\n",
    "service_name = \"ObjectClassification\"\n",
    "\n",
    "# set the path to the models directory\n",
    "model_dir = Path().joinpath(path_to_service, service_name, \"models\")\n",
    "print(f\"Model directory: {model_dir}\")\n",
    "\n",
    "# create the model directory\n",
    "model_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model zip file\n",
    "model_url = \"https://zenodo.org/records/14755172/files/freshwater_phytoplankton_model.zip\"\n",
    "model_file = model_dir.with_suffix(\".zip\")\n",
    "\n",
    "if not model_file.exists():\n",
    "    print(f\"Downloading model from {model_url}\")\n",
    "    r = requests.get(model_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        # get total file size\n",
    "        total_size = int(r.headers.get('content-length', 0))\n",
    "        \n",
    "        # save the model to a file with progress bar\n",
    "        with open(model_file, 'wb') as f, tqdm(\n",
    "            desc=\"Downloading\",\n",
    "            total=total_size,\n",
    "            unit='iB',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as pbar:\n",
    "            for data in r.iter_content(chunk_size=1024):\n",
    "                size = f.write(data)\n",
    "                pbar.update(size)\n",
    "        \n",
    "        # extract the model with progress bar\n",
    "        with zipfile.ZipFile(model_file, 'r') as zip_ref:\n",
    "            members = zip_ref.namelist()\n",
    "            for member in tqdm(members, desc=\"Extracting\"):\n",
    "                zip_ref.extract(member, model_dir)\n",
    "        print(f\"Model extracted to {model_dir}\")\n",
    "\n",
    "        # remove the zip file\n",
    "        model_file.unlink()\n",
    "    else:\n",
    "        print(f\"Failed to download model: {r.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Model loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_configuration(config_file_path: Path) -> ModelConfig:\n",
    "        try:\n",
    "            with open(config_file_path, \"r\") as file:\n",
    "                model_config = json.load(file)\n",
    "                return ModelConfig(model_config=model_config)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Model configuration file is missing required values: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model configuration\n",
    "model_config_file = list(model_dir.glob(\"**/*.json\"))[0]\n",
    "model_config = load_model_configuration(model_config_file)\n",
    "print(f\"Model configuration: {model_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a label encoder to encode that encodes all labels, including those that are not present in the data and model prediction classes\n",
    "encoder = LabelEncoder()\n",
    "target_names = set(input_data_samples[\"LabelTrue\"].unique()).union(set(model_config.Class_names))\n",
    "encoder.fit(list(target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now use this encoder to transform the labels in the data\n",
    "input_data_samples[\"LabelTrue\"] = encoder.transform(input_data_samples[\"LabelTrue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to print label counts\n",
    "def print_label_counts(data: pd.DataFrame, class_names: list[str]):\n",
    "    label_counts = data[\"LabelTrue\"].value_counts()\n",
    "    value_counts = {}\n",
    "    for label, count in label_counts.items():\n",
    "        value_counts[class_names[label]] = count\n",
    "\n",
    "    # sort the labels by count\n",
    "    sorted_value_counts = sorted(\n",
    "        value_counts.items(), key=lambda x: x[1], reverse=False\n",
    "    )\n",
    "    sorted_labels = [label for label, count in sorted_value_counts]\n",
    "    sorted_counts = [count for label, count in sorted_value_counts]\n",
    "\n",
    "    # plot the label counts\n",
    "    px.bar(\n",
    "        x=sorted_counts,\n",
    "        y=sorted_labels,\n",
    "        title=\"Label Counts\",\n",
    "        orientation=\"h\",\n",
    "        labels={\"x\": \"Count\", \"y\": \"Label\"},\n",
    "        width=800,\n",
    "        height=1200,\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"the data contains these labels: {encoder.classes_}; \\na total of {len(encoder.classes_)} labels\"\n",
    ")\n",
    "print_label_counts(input_data_samples, encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a model encoder to encode the model labels because not all labels present in the data are present in the model configuration\n",
    "model_encoder = LabelEncoder()\n",
    "model_encoder.fit(model_config.Class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model_file_path = list(model_dir.glob(\"**/*.keras\"))[0]\n",
    "model = tf.keras.models.load_model(model_file_path)\n",
    "print(f\"Model loaded from {model_file_path}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  read the image file\n",
    "def decode_image(row: pd.Series, image_size: tuple[int, int, int]) -> tf.Tensor:\n",
    "    if \"ImageFilename\" in row:\n",
    "        image_string = tf.io.read_file(row[\"ImageFilename\"])\n",
    "        image = tf.io.decode_png(image_string, channels=image_size[-1])  # png images\n",
    "        return image\n",
    "    else:\n",
    "        image_path = tf.strings.as_string(row[\"CollageFile\"])\n",
    "        image = tf.numpy_function(read_tiff, [image_path], tf.uint8)\n",
    "        image.set_shape([None, None, 3])\n",
    "        image = remove_alpha_channel(\n",
    "            image, image_size=image_size\n",
    "        )  # RGBA (4 channels) to RGB (3 channels)\n",
    "        image = crop_image(row, image)  # crop out the object image\n",
    "        return image\n",
    "\n",
    "\n",
    "# remove the alpha channel\n",
    "def remove_alpha_channel(image, image_size: tuple[int, int, int]) -> tf.Tensor:\n",
    "    return tf.convert_to_tensor(image[:, :, : image_size[-1]])  # remove alpha channel\n",
    "\n",
    "# read TIFF images\n",
    "def read_tiff(path_tensor: tf.Tensor):\n",
    "    # path_tensor is already bytes, just decode it\n",
    "    path = path_tensor.decode(\"utf-8\")\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image not found at path: {path}\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "# crop out the object image from the collage\n",
    "def crop_image(row: pd.Series, image):\n",
    "    image_x = tf.squeeze(row[\"ImageX\"])\n",
    "    image_y = tf.squeeze(row[\"ImageY\"])\n",
    "    image_width = tf.squeeze(row[\"ImageW\"])\n",
    "    image_height = tf.squeeze(row[\"ImageH\"])\n",
    "    return image[\n",
    "        int(image_y) : int(image_y) + int(image_height),\n",
    "        int(image_x) : int(image_x) + int(image_width),\n",
    "    ]\n",
    "\n",
    "\n",
    "def resize_image(image, image_size: tuple[int, int, int]) -> tf.Tensor:\n",
    "    image = tf.image.resize(image, [image_size[0], image_size[1]])  # H, W only\n",
    "    return image\n",
    "\n",
    "\n",
    "# combining all the image processing functions\n",
    "def get_image(row: pd.Series, image_size: tuple[int, int, int]) -> tf.Tensor:\n",
    "    image = decode_image(row, image_size=image_size)\n",
    "    return resize_image(image, image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object features\n",
    "def get_features(row: pd.Series, feature_names: list[str]) -> tf.Tensor:\n",
    "    return tf.convert_to_tensor(\n",
    "        [float(row[feature]) for feature in feature_names], dtype=tf.float64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "def get_label(row: pd.Series):\n",
    "    return row.pop(\"LabelTrue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "    row: pd.Series,\n",
    "    image_size: Tuple[int, int, int],\n",
    "    feature_names: list[str],\n",
    "):\n",
    "    image = get_image(row, image_size=image_size)\n",
    "    features = get_features(row, feature_names=feature_names)\n",
    "    label = get_label(row)\n",
    "    return (\n",
    "        features,\n",
    "        image,\n",
    "    ), label  # Note: the order of the features and image is important for the model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 22\n",
    "\n",
    "# create the training datasets\n",
    "ds = tf.data.Dataset.from_tensor_slices(dict(input_data_samples))\n",
    "ds = ds.map(\n",
    "    lambda x: get_data(x, image_size=model_config.Input_shape, feature_names=model_config.Features),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "ds = (\n",
    "    ds.batch(batch_size=batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. **Model evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = model.predict(ds, verbose=1)\n",
    "predicted_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-encode the predicted labels to ensure that class mapping is consistent\n",
    "inverse_predicted_labels = model_encoder.inverse_transform(predicted_labels)\n",
    "remapped_predicted_labels = encoder.transform(inverse_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target names\n",
    "target_names = [encoder.classes_[i] for i in set(input_data_samples[\"LabelTrue\"].unique()).union(set(remapped_predicted_labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the classification accuracy\n",
    "print(f\"Overall classification accuracy is: {accuracy_score(input_data_samples['LabelTrue'], remapped_predicted_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "def plot_confusion_matrix(\n",
    "    true_labels: np.ndarray,\n",
    "    predicted_labels: np.ndarray,\n",
    "    class_names: list[str],\n",
    "    text_size: int = 10,\n",
    "    normalize: bool = True,\n",
    "    width: int = 1000,\n",
    "    height: int = 1000,\n",
    "):\n",
    "    cm = confusion_matrix(\n",
    "        y_true=true_labels,\n",
    "        y_pred=predicted_labels,\n",
    "        normalize=\"true\" if normalize else None,\n",
    "    )\n",
    "    # normalize the confusion matrix\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=cm,\n",
    "            x=class_names,\n",
    "            y=class_names,\n",
    "            colorscale=\"Viridis\",\n",
    "            showscale=False,\n",
    "            text=cm,\n",
    "            texttemplate=\"%{text:.2f}\",\n",
    "            textfont={\"size\": text_size},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Confusion Matrix\",\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"Predicted\",\n",
    "        yaxis_title=\"True\",\n",
    "        autosize=False,\n",
    "        width=width,\n",
    "        height=height,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    predicted_labels=remapped_predicted_labels,\n",
    "    true_labels=input_data_samples[\"LabelTrue\"],\n",
    "    class_names=target_names,\n",
    "    text_size=10,\n",
    "    normalize=True,\n",
    "    width = 1200,\n",
    "    height = 1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report\n",
    "print(\n",
    "    classification_report(\n",
    "        input_data_samples[\"LabelTrue\"], \n",
    "        remapped_predicted_labels,\n",
    "        target_names=target_names,\n",
    "        zero_division=0\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
